# -*- coding: utf-8 -*-
"""GA3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kv6KfVrUWwvLbBVMqEiLgm1VNZV_YhEg
"""

# Mari Sisco and Tim Lefebvre

# Importing necessary libraries for clustering and plotting
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

from google.colab import drive

# Loading the csv file from google drive to a pandas Dataframe
drive.mount('/content/drive')

dfIncome = pd.read_csv("/content/drive/My Drive/income_utf8.csv")
dfIncome.describe()


# Choosing columns that will be used in analysis
dfIncomeReduced = dfIncome[['ALand','Mean', 'Median', 'Stdev', 'Households']]

# Checking number of missing values
def checkmissing(df):
  for column in df.columns:
    countmissing = 0
    for value in df[column]:
      if value==0:
        countmissing += 1
    print(column , ":" , countmissing)

# Before cleaning, there are a certain number of missing values
print("Number of missing values before cleaning:")
checkmissing(dfIncomeReduced)
# Removing all values where it equals 0
dfIncomeReduced = dfIncomeReduced.loc[(dfIncomeReduced!=0).all(axis=1)]
# After cleaning, no missing values left
print("\nNumber of missing values after cleaning:")
checkmissing(dfIncomeReduced)

# Creating histograms of each column in order to see the skewness of each
dfIncomeReduced.hist(bins=50)

# Copy Of Original
dfIncomeReducedCopy = dfIncomeReduced.copy()

# function used to standardize Dataframe
def standardize_data(df, columns):
    # scaler: initializing StandardScaler
    scaler = StandardScaler()
    # Apply scaler to the specified columns
    df[columns] = scaler.fit_transform(df[columns])

    return df

# Standardizing columns
dfIncomeStandardized = standardize_data(dfIncomeReduced, columns = ['ALand','Mean', 'Median', 'Stdev', 'Households'])

# Weighting the importance of each variable
weights = {
    'ALand': 1,
    'Mean': 30,
    'Median': 3,
    'Stdev': 6,
    'Households': 1
}

# Appling weights
for col in dfIncomeStandardized.columns:
    dfIncomeStandardized[col] *= weights[col]

# Finding best number of clusters

# Code obtained from Statology.org

#initialize kmeans parameters
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"random_state": 1,
}

#create list to hold SSE values for each k
sse = []
KMeansdf = dfIncomeStandardized[['Mean','Median','Stdev']]
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(KMeansdf)
    sse.append(kmeans.inertia_)

#visualize results
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

from sklearn.metrics import silhouette_score

# Perfoming kmeans clustering and evaluating success through the Silhouette Score
def kmeans_clustering(df, dfOriginal, n_clusters):
  # kmeans clustering
  kmeans = KMeans(n_clusters=n_clusters, random_state=42)
  kmeans.fit(df)
  labels = kmeans.labels_
  df.loc[:, 'Cluster'] = labels
  dfOriginal.loc[:, 'Cluster'] = labels

  # Obtaining Silhouette score
  silhouette_avg = silhouette_score(df, labels)
  print("Validity of clustering, Silhouette score:", silhouette_avg)

  df = dfOriginal
  # Returns Dataframe with clusters
  return df

clustered_income_df = kmeans_clustering(KMeansdf, dfIncomeReducedCopy, 3)
clustered_income_df.to_csv('clustered_income_data.csv', index=False)

# Group the DataFrame by the 'Cluster' column
cluster_stats = clustered_income_df.groupby('Cluster').agg(['mean', 'median', 'std']).reset_index()

# Display the descriptive statistics for each cluster
print(cluster_stats)

import seaborn as sns

# Visualize clusters in Mean vs Median Scenario
plt.figure(figsize=(10, 6))
sns.scatterplot(data=clustered_income_df, x='Mean', y='Median', hue='Cluster', palette='viridis', s=100, alpha=0.7)
plt.title('Cluster Visualization')
plt.xlabel('Mean')
plt.ylabel('Median')
plt.grid(True)
plt.savefig('cluster_visualization.png')
plt.show()